{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2faf074",
   "metadata": {},
   "source": [
    "# Method2: Maclurain Random Feature implemented on one row of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a71a20",
   "metadata": {},
   "source": [
    "# Precompute all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aba8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# ——— Load and prepare q0, k0 as NumPy arrays ———\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "\n",
    "q = q_full[0]  # shape [L, d_model]\n",
    "k = k_full[0]\n",
    "\n",
    "L, d_model = q.shape\n",
    "num_heads  = 32\n",
    "d_head     = d_model // num_heads\n",
    "\n",
    "# pick head 15 and first `sample` positions\n",
    "sample = 4096\n",
    "def sampling(q, k, sample):\n",
    "    q0 = (\n",
    "        q\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )   # shape [sample, d_head]\n",
    "    k0 = (\n",
    "        k\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    q0 = q0 / 128 ** 0.25\n",
    "    k0 = k0 / 128 ** 0.25\n",
    "    \n",
    "    return q0, k0\n",
    "\n",
    "\n",
    "def rfa_attention_fast(q0, k0, P=8, D=2000, shrink=10.0, power=100):\n",
    "    \"\"\"\n",
    "    q0, k0:  (N, d) arrays of queries & keys\n",
    "    returns: (N, N) approximate softmax(QK^T)\n",
    "    \"\"\"\n",
    "    # 1) Pre-scale and cast to float32\n",
    "    X = (q0 / shrink).astype(np.float64)   # (N, d)\n",
    "    Y = (k0 / shrink).astype(np.float64)\n",
    "\n",
    "    N, d = X.shape\n",
    "\n",
    "    # 2) Sample ±1 weights in one flat block, cast to float32\n",
    "    #    shape = (P*D, d)\n",
    "    w_flat = np.sign(np.random.randn(P * D, d)).astype(np.float64)\n",
    "\n",
    "    # 3) One mat-mul to get all projections, then reshape to (N,P,D)\n",
    "    #    proj_x[n, p*D + j] = w_flat[p*D + j] ⋅ X[n]\n",
    "    proj_x_flat = X.dot(w_flat.T)           # (N, P*D)\n",
    "    proj_y_flat = Y.dot(w_flat.T)\n",
    "\n",
    "    proj_x = proj_x_flat.reshape(N, P, D)   # (N, P, D)\n",
    "    proj_y = proj_y_flat.reshape(N, P, D)\n",
    "\n",
    "    # 4) Build per-degree normalizers √(D·p!) for p=1..P\n",
    "    facts = np.array([math.sqrt(math.factorial(p+1)) for p in range(P)],\n",
    "                     dtype=np.float64)     # (P,)\n",
    "    normalizer = np.sqrt(D, dtype=np.float64) * facts\n",
    "    normalizer = normalizer.reshape(1, P, 1)  # (1, P, 1)\n",
    "\n",
    "    # 5) Cumulative product along the P-axis to get φ_p\n",
    "    #    φ_p = ∏_{m=1..p} (proj[...,m-1] / normalizer[...,m-1])\n",
    "    phi_x = np.cumprod(proj_x / normalizer, axis=1)  # (N, P, D)\n",
    "    phi_y = np.cumprod(proj_y / normalizer, axis=1)\n",
    "\n",
    "    # 6) Flatten φ back to (N, P*D)\n",
    "    phi_x_flat = phi_x.reshape(N, P * D)\n",
    "    phi_y_flat = phi_y.reshape(N, P * D)\n",
    "\n",
    "    # 7) One big BLAS mat-mul to form the kernel matrix\n",
    "    S = phi_x_flat.dot(phi_y_flat.T)  # (N, N)\n",
    "\n",
    "    # 8) Sharpen & row-normalize\n",
    "    M = (1.0 + S) ** power\n",
    "    M /= M.max(axis=1, keepdims=True)\n",
    "    M /= M.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# usage:\n",
    "# approx = rfa_attention_vectorized(q0, k0)\n",
    "\n",
    "def true_softmax(q0, k0):\n",
    "    dot = q0 @ k0.T\n",
    "    true_val = np.exp(dot - dot.max(axis=1, keepdims=True))\n",
    "    true_val /= true_val.sum(axis=1, keepdims=True)\n",
    "    return true_val\n",
    "\n",
    "def report_error(record_approx_values, true_val):\n",
    "    return torch.norm(torch.tensor(record_approx_values - true_val)) / torch.norm(torch.tensor(true_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99cd7",
   "metadata": {},
   "source": [
    "# Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa70ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, D, d = 4, 2000, 128\n",
    "sample=4096\n",
    "q0, k0 = sampling(q, k, sample)\n",
    "v0 =(q0+ 4 * k0) /3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfe3e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_approx_values = rfa_attention_fast(q0, k0, P, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad27dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_v=record_approx_values @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7bfdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = true_softmax(q0, k0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = true_vals @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7787d576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2413, dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(record_approx_values - true_vals)) / torch.norm(torch.tensor(true_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a1ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09c7e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0297, dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(approx_v - true_val)) / torch.norm(torch.tensor(true_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9152d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFAMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 P: int = 8,\n",
    "                 D: int = 2000,\n",
    "                 shrink: float = 10.0, \n",
    "                 power: float = 100.0,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.P = P\n",
    "        self.D = D\n",
    "        self.shrink = shrink\n",
    "        self.power = power\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 推迟随机特征的初始化，直到我们知道输入类型\n",
    "        self.random_weights = None\n",
    "        self.normalizer = None\n",
    "        \n",
    "    def _init_random_features(self, dtype, device):\n",
    "        \"\"\"初始化每个头的随机特征权重，根据输入的dtype和device\"\"\"\n",
    "        self.random_weights = nn.Parameter(\n",
    "            torch.randn(self.num_heads, self.P * self.D, self.d_head, \n",
    "                       dtype=dtype, device=device),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        # 将权重转换为±1\n",
    "        self.random_weights.data.sign_()\n",
    "        \n",
    "        # 预计算归一化因子\n",
    "        facts = torch.tensor([math.sqrt(math.factorial(p+1)) for p in range(self.P)],\n",
    "                            dtype=dtype, device=device)\n",
    "        self.normalizer = torch.sqrt(torch.tensor(self.D, dtype=dtype, device=device)) * facts.view(1, -1, 1)\n",
    "        \n",
    "    def _rfa_attention_head(self, q, k, v, head_idx):\n",
    "        # 确保初始化了随机权重\n",
    "        if self.random_weights is None:\n",
    "            self._init_random_features(q.dtype, q.device)\n",
    "        \n",
    "        batch_size, seq_len, _ = q.shape\n",
    "        \n",
    "        # 确保所有计算都使用相同的dtype\n",
    "        X = q / self.shrink\n",
    "        Y = k / self.shrink\n",
    "        \n",
    "        # 获取并确保相同dtype的随机权重\n",
    "        w_flat = self.random_weights[head_idx]\n",
    "        \n",
    "        # 确认类型匹配\n",
    "        assert X.dtype == w_flat.dtype, f\"类型不匹配: X.dtype={X.dtype}, w_flat.dtype={w_flat.dtype}\"\n",
    "        \n",
    "        # 执行计算\n",
    "        proj_x_flat = X @ w_flat.t()\n",
    "        proj_y_flat = Y @ w_flat.t()\n",
    "        \n",
    "        proj_x = proj_x_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        proj_y = proj_y_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        \n",
    "        phi_x = torch.cumprod(proj_x / self.normalizer, dim=2)\n",
    "        phi_y = torch.cumprod(proj_y / self.normalizer, dim=2)\n",
    "        \n",
    "        phi_x_flat = phi_x.view(batch_size, seq_len, -1)\n",
    "        phi_y_flat = phi_y.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        S = phi_x_flat @ phi_y_flat.transpose(-2, -1)\n",
    "        \n",
    "        M = (1.0 + S).pow(self.power)\n",
    "        M = M / M.max(dim=-1, keepdim=True)[0]\n",
    "        attn = self.dropout(M / M.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        return attn @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e12101ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "速度测试结果:\n",
      "RFA Attention: <torch.utils.benchmark.utils.common.Measurement object at 0x7f143d329360>\n",
      "benchmark_rfa()\n",
      "  43.17 ms\n",
      "  1 measurement, 10 runs , 1 thread\n",
      "官方MultiheadAttention: <torch.utils.benchmark.utils.common.Measurement object at 0x7f143d328af0>\n",
      "benchmark_torch_mha()\n",
      "  1.53 ms\n",
      "  1 measurement, 10 runs , 1 thread\n",
      "\n",
      "相对L2误差: 1.217e-01\n",
      "最大绝对误差: 8.446e-01\n",
      "平均绝对误差: 4.437e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "# 假设RFAMultiHeadAttention已在同目录下定义好\n",
    "\n",
    "def test_rfa_vs_torch_mha():\n",
    "    # 1. 加载数据\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\", map_location=device)\n",
    "    k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\", map_location=device)\n",
    "    \n",
    "    sample_size = 4096\n",
    "    d_model = 4096\n",
    "    num_heads = 32\n",
    "    d_head = d_model // num_heads\n",
    "    head_idx = 15\n",
    "\n",
    "    def prepare_test_data():\n",
    "        q = q_full[0].float()\n",
    "        k = k_full[0].float()\n",
    "        # 取出指定head\n",
    "        q0 = (q.view(-1, num_heads, d_head)\n",
    "              .permute(1, 0, 2)[head_idx, :sample_size]\n",
    "              .div_(128 ** 0.25)\n",
    "              .to(device))\n",
    "        k0 = (k.view(-1, num_heads, d_head)\n",
    "              .permute(1, 0, 2)[head_idx, :sample_size]\n",
    "              .div_(128 ** 0.25)\n",
    "              .to(device))\n",
    "        v0 = ((q0 + 4 * k0) / 3).float()\n",
    "        # 变成(seq_len, batch, embed_dim)格式，batch=1\n",
    "        return q0.unsqueeze(1), k0.unsqueeze(1), v0.unsqueeze(1)\n",
    "\n",
    "    # 2. 初始化RFA和官方MHA\n",
    "    rfa_model = RFAMultiHeadAttention(\n",
    "        d_model=d_head,\n",
    "        num_heads=1,\n",
    "        P=8,\n",
    "        D=2000,\n",
    "        shrink=10.0,\n",
    "        power=100.0\n",
    "    ).to(device)\n",
    "    torch_mha = torch.nn.MultiheadAttention(\n",
    "        embed_dim=d_head,\n",
    "        num_heads=1,\n",
    "        batch_first=False,\n",
    "        bias=False\n",
    "    ).to(device)\n",
    "    # 让MHA的权重为单位阵，等价于直接qk^T\n",
    "    torch_mha.in_proj_weight.data.copy_(torch.cat([\n",
    "        torch.eye(d_head), torch.eye(d_head), torch.eye(d_head)\n",
    "    ], dim=0))\n",
    "\n",
    "    torch_mha.out_proj.weight.data.copy_(torch.eye(d_head))\n",
    "\n",
    "\n",
    "    # 3. 定义基准函数，每次都重新加载qkv\n",
    "    def benchmark_rfa():\n",
    "        q0, k0, v0 = prepare_test_data()\n",
    "        # (1, seq, d_head) -> (batch, seq, d_head)\n",
    "        return rfa_model._rfa_attention_head(\n",
    "            q0.permute(1,0,2), k0.permute(1,0,2), v0.permute(1,0,2), head_idx=0\n",
    "        ).squeeze(0)  # (seq, d_head)\n",
    "\n",
    "    def benchmark_torch_mha():\n",
    "        q0, k0, v0 = prepare_test_data()\n",
    "        # (seq, batch, d_head)\n",
    "        out, _ = torch_mha(q0, k0, v0, need_weights=False)\n",
    "        return out.squeeze(1)  # (seq, d_head)\n",
    "\n",
    "    # 4. 预热\n",
    "    for _ in range(3):\n",
    "        benchmark_rfa()\n",
    "        benchmark_torch_mha()\n",
    "\n",
    "    # 5. 速度测试\n",
    "    timer_rfa = Timer(\n",
    "        stmt=\"benchmark_rfa()\",\n",
    "        globals={'benchmark_rfa': benchmark_rfa}\n",
    "    )\n",
    "    timer_torch = Timer(\n",
    "        stmt=\"benchmark_torch_mha()\",\n",
    "        globals={'benchmark_torch_mha': benchmark_torch_mha}\n",
    "    )\n",
    "    print(\"\\n速度测试结果:\")\n",
    "    print(f\"RFA Attention: {timer_rfa.timeit(10)}\")\n",
    "    print(f\"官方MultiheadAttention: {timer_torch.timeit(10)}\")\n",
    "\n",
    "    # 6. 精度测试\n",
    "    with torch.no_grad():\n",
    "        rfa_out = benchmark_rfa()\n",
    "        torch_out = benchmark_torch_mha()\n",
    "        # 计算相对L2误差\n",
    "        error = torch.norm(rfa_out - torch_out) / torch.norm(torch_out)\n",
    "        print(f\"\\n相对L2误差: {error.item():.3e}\")\n",
    "        print(f\"最大绝对误差: {torch.max(torch.abs(rfa_out - torch_out)).item():.3e}\")\n",
    "        print(f\"平均绝对误差: {torch.mean(torch.abs(rfa_out - torch_out)).item():.3e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rfa_vs_torch_mha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6fb45",
   "metadata": {},
   "source": [
    "\n",
    "速度测试结果:\n",
    "RFA Attention: <torch.utils.benchmark.utils.common.Measurement object at 0x7f143d329360>\n",
    "benchmark_rfa()\n",
    "  43.17 ms\n",
    "  1 measurement, 10 runs , 1 thread\n",
    "官方MultiheadAttention: <torch.utils.benchmark.utils.common.Measurement object at 0x7f143d328af0>\n",
    "benchmark_torch_mha()\n",
    "  1.53 ms\n",
    "  1 measurement, 10 runs , 1 thread\n",
    "\n",
    "相对L2误差: 1.217e-01\n",
    "最大绝对误差: 8.446e-01\n",
    "平均绝对误差: 4.437e-02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
