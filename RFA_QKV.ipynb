{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load your q0 and k0 from the provided files\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\")\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\")\n",
    "q = q_full[0]\n",
    "k = k_full[0]\n",
    "L, d_model = q.shape\n",
    "num_heads = 32\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "# Extract head 15\n",
    "q0 = q.view(L, num_heads, d_head).permute(1, 0, 2)[15]\n",
    "k0 = k.view(L, num_heads, d_head).permute(1, 0, 2)[15]\n",
    "# Normalize as per the paper\n",
    "q0 = q0 / (128**0.25)\n",
    "k0 = k0 / (128**0.25)\n",
    "\n",
    "# Convert to NumPy\n",
    "q0_np = q0.cpu().numpy()\n",
    "k0_np = k0.cpu().numpy()\n",
    "V_np = (q0_np + 3* k0_np) /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 0, 0\n",
    "for i in range(4096):\n",
    "    nx_i = np.linalg.norm(q0_np[i,:])\n",
    "    ny_i = np.linalg.norm(k0_np[i,:])\n",
    "    if nx_i > nx:\n",
    "        nx = nx_i\n",
    "    if ny_i > ny:\n",
    "        ny = ny_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2faf074",
   "metadata": {},
   "source": [
    "# Method2: Maclurain Random Feature implemented on one row of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "P = 8\n",
    "D = 2000\n",
    "d = 128\n",
    "\n",
    "\n",
    "sample = 4096\n",
    "\n",
    "# Precompute phi_x\n",
    "norm_x = np.zeros((sample))\n",
    "phi_x = np.zeros((P, D))\n",
    "w = np.where(np.random.rand(P, D, d) < 0.5, -1.0, +1.0)\n",
    "pos = 200\n",
    "x = np.array(q0[pos,:])\n",
    "nx = 1\n",
    "x/=nx\n",
    "for p in range(1, P+1):\n",
    "    for j in range(D):\n",
    "        scale = 1 / np.sqrt(D)               \n",
    "        for i in range(p):\n",
    "            scale *= np.dot(w[i, j], x)\n",
    "        phi_x[p-1, j] = scale \n",
    "\n",
    "            \n",
    "approx_vals = [] \n",
    "for i in range(sample):\n",
    "    y = np.array(k0[i,:]) \n",
    "    ny = 1\n",
    "    y/=ny\n",
    "    # Compute phi_y\n",
    "    phi_y = np.zeros((P, D))\n",
    "    for p in range(1, P+1):\n",
    "        for j in range(D):\n",
    "            scale = 1 / np.sqrt(D)\n",
    "            for i in range(p):\n",
    "                scale *= np.dot(w[i, j], y)\n",
    "            phi_y[p-1, j] = scale\n",
    "    # Approximate kernel\n",
    "    approx_k = 1.0\n",
    "    last_postive = 0.0\n",
    "    for p in range(P):\n",
    "        approx_k += abs(np.dot(phi_x[p], phi_y[p])) / math.factorial(p+1)\n",
    "    approx_k = approx_k ** (nx * ny)\n",
    "    approx_vals.append(approx_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b936264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "P = 8\n",
    "D = 2000\n",
    "d = 128\n",
    "\n",
    "\n",
    "sample = 4096\n",
    "\n",
    "# Precompute phi_x\n",
    "phi_x_record= np.zeros((sample, P, D))\n",
    "phi_x = np.zeros((P, D))\n",
    "w = np.where(np.random.rand(P, D, d) < 0.5, -1.0, +1.0)\n",
    "pos = 200\n",
    "\n",
    "Z = (q0 + k0) / 2 # testing v\n",
    "Z_100th = Z ** 0.01\n",
    "# Let function f be row norm such that making row sum be 1 so f(exp) = softmax\n",
    "# My implmention is softmax approxition without V(Z), softmax((Q) X dot (K)Y ) begin with exp(X dot Y) approxiamation using\n",
    "# random feature where  exp(XdotY) = exp(X/10 dot Y/10) ** 100  =  exp(x, y) ** 100 and having a f on each row to get softmax approximation\n",
    "# original function is softmax(X dot Y) Z = f(exp(X dot Y)) Z= f((phi x dot phi y) ** 100) dot Z   = f'(((phi x dot phi y dot Z_100th) ** 100))\n",
    "# f' is not calculate the same way as f Please get it\n",
    "# phi x dot phi y dot Z_100th can be calculate quickly as y dot Z_100th 's result can be reused\n",
    "# make a power after all finished \n",
    "\n",
    "for i in range(sample):\n",
    "    x = np.array(q0[pos,:])\n",
    "    # 10 is scalar factor to make random feature valid\n",
    "    x/=10\n",
    "    for p in range(1, P+1):\n",
    "        for j in range(D):\n",
    "            scale = 1 / np.sqrt(D)               \n",
    "            for ps in range(p):\n",
    "                scale *= np.dot(w[ps, j], x)\n",
    "            phi_x[p-1, j] = scale \n",
    "    phi_x_record[i] = phi_x\n",
    "\n",
    "approx_record = np.zeros((sample, sample))            \n",
    "\n",
    "for i in range(sample):\n",
    "    y = np.array(k0[i,:]) \n",
    "    y/=10\n",
    "    # Compute phi_y\n",
    "    phi_y = np.zeros((P, D))\n",
    "    for p in range(1, P+1):\n",
    "        for j in range(D):\n",
    "            scale = 1 / np.sqrt(D)\n",
    "            for ps in range(p):\n",
    "                scale *= np.dot(w[ps, j], y)\n",
    "            phi_y[p-1, j] = scale\n",
    "    \n",
    "for i in range(sample):\n",
    "    approx_vals = np.zeros((sample))\n",
    "    for j in range(sample):\n",
    "        # Approximate kernel\n",
    "        approx_k = 1.0\n",
    "        last_postive = 0.0\n",
    "        for p in range(P):\n",
    "            approx_k += abs(np.dot(phi_x[p], phi_y[p])) / math.factorial(p+1)\n",
    "        approx_k = approx_k ** (100)\n",
    "        approx_k /= approx_k.max()\n",
    "        approx_k /= approx_k.sum()\n",
    "        approx_vals[j] = approx_k\n",
    "    approx_record[i] = approx_vals\n",
    "\n",
    "# Normal calculation is approx_record product \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(q0[pos,:])\n",
    "true_vals = []\n",
    "sample = 40\n",
    "for i in range(sample):\n",
    "    y = np.array(k0[i,:]) \n",
    "    true_k = np.exp(np.dot(x, y))\n",
    "    true_vals.append(true_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10198397",
   "metadata": {},
   "source": [
    "#### Normalized to ensure numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals /= max(true_vals)\n",
    "true_vals /= sum(true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_vals /= max(approx_vals)\n",
    "approx_vals /= sum(approx_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(torch.tensor(true_vals) - torch.tensor(approx_vals)) / torch.norm(torch.tensor(true_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96138d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(40)\n",
    "s1 = 0\n",
    "begin, end = s1 * 40, s1*4000 + 400\n",
    "sample = 4096\n",
    "# 4) plot\n",
    "plt.figure()\n",
    "plt.plot(x[begin:end], true_vals[begin:end],    label='True exp(xᵀy)')\n",
    "plt.plot(x[begin:end], approx_vals[begin:end],  label='Approx')\n",
    "plt.ylabel('Kernel value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a71a20",
   "metadata": {},
   "source": [
    "# Precompute all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aba8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# ——— Load and prepare q0, k0 as NumPy arrays ———\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "\n",
    "q = q_full[0]  # shape [L, d_model]\n",
    "k = k_full[0]\n",
    "\n",
    "L, d_model = q.shape\n",
    "num_heads  = 32\n",
    "d_head     = d_model // num_heads\n",
    "\n",
    "# pick head 15 and first `sample` positions\n",
    "sample = 4096\n",
    "def sampling(q, k, sample):\n",
    "    q0 = (\n",
    "        q\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )   # shape [sample, d_head]\n",
    "    k0 = (\n",
    "        k\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    q0 = q0 / 128 ** 0.25\n",
    "    k0 = k0 / 128 ** 0.25\n",
    "    \n",
    "    return q0, k0\n",
    "\n",
    "\n",
    "def rfa_attention_fast(q0, k0, P=8, D=2000, shrink=10.0, power=100):\n",
    "    \"\"\"\n",
    "    q0, k0:  (N, d) arrays of queries & keys\n",
    "    returns: (N, N) approximate softmax(QK^T)\n",
    "    \"\"\"\n",
    "    # 1) Pre-scale and cast to float32\n",
    "    X = (q0 / shrink).astype(np.float64)   # (N, d)\n",
    "    Y = (k0 / shrink).astype(np.float64)\n",
    "\n",
    "    N, d = X.shape\n",
    "\n",
    "    # 2) Sample ±1 weights in one flat block, cast to float32\n",
    "    #    shape = (P*D, d)\n",
    "    w_flat = np.sign(np.random.randn(P * D, d)).astype(np.float64)\n",
    "\n",
    "    # 3) One mat-mul to get all projections, then reshape to (N,P,D)\n",
    "    #    proj_x[n, p*D + j] = w_flat[p*D + j] ⋅ X[n]\n",
    "    proj_x_flat = X.dot(w_flat.T)           # (N, P*D)\n",
    "    proj_y_flat = Y.dot(w_flat.T)\n",
    "\n",
    "    proj_x = proj_x_flat.reshape(N, P, D)   # (N, P, D)\n",
    "    proj_y = proj_y_flat.reshape(N, P, D)\n",
    "\n",
    "    # 4) Build per-degree normalizers √(D·p!) for p=1..P\n",
    "    facts = np.array([math.sqrt(math.factorial(p+1)) for p in range(P)],\n",
    "                     dtype=np.float64)     # (P,)\n",
    "    normalizer = np.sqrt(D, dtype=np.float64) * facts\n",
    "    normalizer = normalizer.reshape(1, P, 1)  # (1, P, 1)\n",
    "\n",
    "    # 5) Cumulative product along the P-axis to get φ_p\n",
    "    #    φ_p = ∏_{m=1..p} (proj[...,m-1] / normalizer[...,m-1])\n",
    "    phi_x = np.cumprod(proj_x / normalizer, axis=1)  # (N, P, D)\n",
    "    phi_y = np.cumprod(proj_y / normalizer, axis=1)\n",
    "\n",
    "    # 6) Flatten φ back to (N, P*D)\n",
    "    phi_x_flat = phi_x.reshape(N, P * D)\n",
    "    phi_y_flat = phi_y.reshape(N, P * D)\n",
    "\n",
    "    # 7) One big BLAS mat-mul to form the kernel matrix\n",
    "    S = phi_x_flat.dot(phi_y_flat.T)  # (N, N)\n",
    "\n",
    "    # 8) Sharpen & row-normalize\n",
    "    M = (1.0 + S) ** power\n",
    "    M /= M.max(axis=1, keepdims=True)\n",
    "    M /= M.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# usage:\n",
    "# approx = rfa_attention_vectorized(q0, k0)\n",
    "\n",
    "def true_softmax(q0, k0):\n",
    "    dot = q0 @ k0.T\n",
    "    true_val = np.exp(dot - dot.max(axis=1, keepdims=True))\n",
    "    true_val /= true_val.sum(axis=1, keepdims=True)\n",
    "    return true_val\n",
    "\n",
    "def report_error(record_approx_values, true_val):\n",
    "    return torch.norm(torch.tensor(record_approx_values - true_val)) / torch.norm(torch.tensor(true_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99cd7",
   "metadata": {},
   "source": [
    "# Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aaa70ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, D, d = 4, 2000, 128\n",
    "sample=4096\n",
    "q0, k0 = sampling(q, k, sample)\n",
    "v0 = q0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cfe3e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_approx_values = rfa_attention_fast(q0, k0, P, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ad27dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_v=record_approx_values @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e7bfdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = true_softmax(q0, k0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c2d70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = true_vals @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7787d576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2544, dtype=torch.float64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(record_approx_values - true_vals)) / torch.norm(torch.tensor(true_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "09c7e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0320, dtype=torch.float64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(approx_v - true_val)) / torch.norm(torch.tensor(true_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
