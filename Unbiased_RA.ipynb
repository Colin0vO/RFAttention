{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load your q0 and k0 from the provided files\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\")\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\")\n",
    "q = q_full[0]\n",
    "k = k_full[0]\n",
    "L, d_model = q.shape\n",
    "num_heads = 32\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "# Extract head 15\n",
    "q0 = q.view(L, num_heads, d_head).permute(1, 0, 2)[15]\n",
    "k0 = k.view(L, num_heads, d_head).permute(1, 0, 2)[15]\n",
    "# Normalize as per the paper\n",
    "q0 = q0 / (128**0.25)\n",
    "k0 = k0 / (128**0.25)\n",
    "\n",
    "# Convert to NumPy\n",
    "q0_np = q0.cpu().numpy()\n",
    "k0_np = k0.cpu().numpy()\n",
    "V_np = (3 * q0 + 4* k0) /5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec94e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.861   ,  0.685   ,  0.4485  , ..., -0.0437  ,  0.0668  ,\n",
       "        -0.01913 ],\n",
       "       [ 0.5903  ,  0.6245  ,  0.6494  , ...,  0.1321  ,  0.2988  ,\n",
       "        -0.0903  ],\n",
       "       [ 0.5063  ,  0.489   ,  0.3208  , ..., -0.06186 ,  0.01463 ,\n",
       "        -0.05743 ],\n",
       "       ...,\n",
       "       [ 0.6494  ,  0.1428  , -0.2837  , ..., -0.1804  ,  0.2925  ,\n",
       "        -0.03418 ],\n",
       "       [ 0.5015  ,  0.2883  , -0.09296 , ..., -0.1843  , -0.001465,\n",
       "        -0.1592  ],\n",
       "       [ 0.4395  ,  0.285   , -0.1267  , ..., -0.25    , -0.0881  ,\n",
       "        -0.2866  ]], dtype=float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9db0cc",
   "metadata": {},
   "source": [
    "# Method1: Unbiased Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fddade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_approx.shape: (4096, 4096)\n",
      "out_approx.shape:   (4096, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_feature_attention(Q, K, V, D=20000, seed=42):\n",
    "    \"\"\"\n",
    "    Approximate softmax-attention using D random‐feature samples.\n",
    "    Q: (L, d), K: (L, d), V: (L, dv)\n",
    "    Returns:\n",
    "      alpha_hat: (L, L)    approximate attention weights\n",
    "      out:       (L, dv)   approximate attended outputs\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    L, d = Q.shape\n",
    "\n",
    "    # 1) draw random weights W ~ N(0,1), shape (D, d)\n",
    "    W = rng.randn(D, d)  # (D, d)\n",
    "\n",
    "    # 2) compute features φ_Q and φ_K:\n",
    "    #    φ(x) = exp(x·Wᵀ − ½‖x‖²) / √D\n",
    "    Q2 = np.sum(Q*Q, axis=1, keepdims=True)   # (L,1)\n",
    "    K2 = np.sum(K*K, axis=1, keepdims=True)   # (L,1)\n",
    "    phi_Q = np.exp(Q.dot(W.T) - 0.5*Q2) / np.sqrt(D)  # (L, D)\n",
    "    phi_K = np.exp(K.dot(W.T) - 0.5*K2) / np.sqrt(D)  # (L, D)\n",
    "\n",
    "    # 3) approximate kernel: φ_Q φ_Kᵀ ≈ exp(Q Kᵀ)\n",
    "    A_hat = phi_Q.dot(phi_K.T)                # (L, L)\n",
    "\n",
    "    # 4) row‐normalize to get weights\n",
    "    alpha_hat = A_hat / A_hat.sum(axis=1, keepdims=True)  # (L, L)\n",
    "\n",
    "    # 5) attended outputs\n",
    "    out = alpha_hat.dot(V)                    # (L, dv)\n",
    "    return alpha_hat, out\n",
    "\n",
    "# ────────────────────────────────────────────────────────\n",
    "# Usage:\n",
    "# (make sure q0_np, k0_np are already NumPy arrays in your session)\n",
    "\n",
    "D_features = 2000\n",
    "alpha_approx, out_approx = random_feature_attention(\n",
    "    q0_np, k0_np, V_np, D=D_features, seed=0\n",
    ")\n",
    "\n",
    "print(\"alpha_approx.shape:\", alpha_approx.shape)  # → (4096, 4096)\n",
    "print(\"out_approx.shape:  \", out_approx.shape)    # → (4096, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0265b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_full.shape: (4096, 4096)\n",
      "out_full.shape:   (4096, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch  # only for loading your .pt files\n",
    "\n",
    "\n",
    "\n",
    "# 2) Exact softmax-attention (NumPy vectorized)\n",
    "scores = q0_np.dot(k0_np.T)                          # (4096,4096)\n",
    "exp_scores = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
    "alpha_full = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (4096,4096)\n",
    "out_full = alpha_full.dot(V_np)                  # (4096,128)\n",
    "\n",
    "print(\"alpha_full.shape:\", alpha_full.shape)    # -> (4096,4096)\n",
    "print(\"out_full.shape:  \", out_full.shape)      # -> (4096,128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef4784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float16(inf)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(out_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9068f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(out_approx - out_full) / np.linalg.norm(out_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc43940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float16(inf)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(out_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
