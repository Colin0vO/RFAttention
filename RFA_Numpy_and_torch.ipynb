{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2faf074",
   "metadata": {},
   "source": [
    "# Method2: Maclurain Random Feature implemented on one row of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a71a20",
   "metadata": {},
   "source": [
    "# Precompute all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# ——— Load and prepare q0, k0 as NumPy arrays ———\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\", map_location=\"cpu\")\n",
    "\n",
    "q = q_full[0]  # shape [L, d_model]\n",
    "k = k_full[0]\n",
    "\n",
    "L, d_model = q.shape\n",
    "num_heads  = 32\n",
    "d_head     = d_model // num_heads\n",
    "\n",
    "# pick head 15 and first `sample` positions\n",
    "sample = 4096\n",
    "def sampling(q, k, sample):\n",
    "    q0 = (\n",
    "        q\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )   # shape [sample, d_head]\n",
    "    k0 = (\n",
    "        k\n",
    "        .view(L, num_heads, d_head)\n",
    "        .permute(1, 0, 2)[15, :sample]\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    q0 = q0 / 128 ** 0.25\n",
    "    k0 = k0 / 128 ** 0.25\n",
    "    \n",
    "    return q0, k0\n",
    "\n",
    "\n",
    "def rfa_attention_fast(q0, k0, P=8, D=2000, shrink=10.0, power=100):\n",
    "    \"\"\"\n",
    "    q0, k0:  (N, d) arrays of queries & keys\n",
    "    returns: (N, N) approximate softmax(QK^T)\n",
    "    \"\"\"\n",
    "    # 1) Pre-scale and cast to float32\n",
    "    X = (q0 / shrink).astype(np.float64)   # (N, d)\n",
    "    Y = (k0 / shrink).astype(np.float64)\n",
    "\n",
    "    N, d = X.shape\n",
    "\n",
    "    # 2) Sample ±1 weights in one flat block, cast to float32\n",
    "    #    shape = (P*D, d)\n",
    "    w_flat = np.sign(np.random.randn(P * D, d)).astype(np.float64)\n",
    "\n",
    "    # 3) One mat-mul to get all projections, then reshape to (N,P,D)\n",
    "    #    proj_x[n, p*D + j] = w_flat[p*D + j] ⋅ X[n]\n",
    "    proj_x_flat = X.dot(w_flat.T)           # (N, P*D)\n",
    "    proj_y_flat = Y.dot(w_flat.T)\n",
    "\n",
    "    proj_x = proj_x_flat.reshape(N, P, D)   # (N, P, D)\n",
    "    proj_y = proj_y_flat.reshape(N, P, D)\n",
    "\n",
    "    # 4) Build per-degree normalizers √(D·p!) for p=1..P\n",
    "    facts = np.array([math.sqrt(math.factorial(p+1)) for p in range(P)],\n",
    "                     dtype=np.float64)     # (P,)\n",
    "    normalizer = np.sqrt(D, dtype=np.float64) * facts\n",
    "    normalizer = normalizer.reshape(1, P, 1)  # (1, P, 1)\n",
    "\n",
    "    # 5) Cumulative product along the P-axis to get φ_p\n",
    "    #    φ_p = ∏_{m=1..p} (proj[...,m-1] / normalizer[...,m-1])\n",
    "    phi_x = np.cumprod(proj_x / normalizer, axis=1)  # (N, P, D)\n",
    "    phi_y = np.cumprod(proj_y / normalizer, axis=1)\n",
    "\n",
    "    # 6) Flatten φ back to (N, P*D)\n",
    "    phi_x_flat = phi_x.reshape(N, P * D)\n",
    "    phi_y_flat = phi_y.reshape(N, P * D)\n",
    "\n",
    "    # 7) One big BLAS mat-mul to form the kernel matrix\n",
    "    S = phi_x_flat.dot(phi_y_flat.T)  # (N, N)\n",
    "\n",
    "    # 8) Sharpen & row-normalize\n",
    "    M = (1.0 + S) ** power\n",
    "    M /= M.max(axis=1, keepdims=True)\n",
    "    M /= M.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "# usage:\n",
    "# approx = rfa_attention_vectorized(q0, k0)\n",
    "\n",
    "def true_softmax(q0, k0):\n",
    "    dot = q0 @ k0.T\n",
    "    true_val = np.exp(dot - dot.max(axis=1, keepdims=True))\n",
    "    true_val /= true_val.sum(axis=1, keepdims=True)\n",
    "    return true_val\n",
    "\n",
    "def report_error(record_approx_values, true_val):\n",
    "    return torch.norm(torch.tensor(record_approx_values - true_val)) / torch.norm(torch.tensor(true_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99cd7",
   "metadata": {},
   "source": [
    "# Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa70ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, D, d = 4, 2000, 128\n",
    "sample=4096\n",
    "q0, k0 = sampling(q, k, sample)\n",
    "v0 =(q0+ 4 * k0) /3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_approx_values = rfa_attention_fast(q0, k0, P, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_v=record_approx_values @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = true_softmax(q0, k0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d70763",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = true_vals @ v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(record_approx_values - true_vals)) / torch.norm(torch.tensor(true_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a1ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.norm(torch.tensor(approx_v - true_val)) / torch.norm(torch.tensor(true_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b43322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class OptimizedRFAMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 P: int = 8,\n",
    "                 D: int = 2000,\n",
    "                 shrink: float = 10.0, \n",
    "                 power: float = 100.0,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.P = P\n",
    "        self.D = D\n",
    "        self.shrink = shrink\n",
    "        self.power = power\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 推迟随机特征的初始化，直到我们知道输入类型\n",
    "        self.random_weights = None\n",
    "        self.normalizer = None\n",
    "        \n",
    "        # 用于存储预计算的kv特征\n",
    "        self.kv_cache = {}\n",
    "        \n",
    "    def _init_random_features(self, dtype, device):\n",
    "        \"\"\"初始化每个头的随机特征权重，根据输入的dtype和device\"\"\"\n",
    "        self.random_weights = nn.Parameter(\n",
    "            torch.randn(self.num_heads, self.P * self.D, self.d_head, \n",
    "                       dtype=dtype, device=device),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        # 将权重转换为±1\n",
    "        self.random_weights.data.sign_()\n",
    "        \n",
    "        # 预计算归一化因子\n",
    "        facts = torch.tensor([math.sqrt(math.factorial(p+1)) for p in range(self.P)],\n",
    "                            dtype=dtype, device=device)\n",
    "        self.normalizer = torch.sqrt(torch.tensor(self.D, dtype=dtype, device=device)) * facts.view(1, -1, 1)\n",
    "    \n",
    "    def _precompute_kv_features(self, k, v, head_idx):\n",
    "        \"\"\"预计算k和v的特征\"\"\"\n",
    "        if self.random_weights is None:\n",
    "            self._init_random_features(k.dtype, k.device)\n",
    "        \n",
    "        batch_size, seq_len, _ = k.shape\n",
    "        \n",
    "        # 确保所有计算都使用相同的dtype\n",
    "        Y = k / self.shrink\n",
    "        w_flat = self.random_weights[head_idx]\n",
    "        \n",
    "        # 执行计算\n",
    "        proj_y_flat = Y @ w_flat.t()\n",
    "        proj_y = proj_y_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        \n",
    "        # 计算phi(k)\n",
    "        phi_y = torch.cumprod(proj_y / self.normalizer, dim=2)\n",
    "        phi_y_flat = phi_y.reshape(batch_size, seq_len, self.P * self.D)\n",
    "        \n",
    "        # 计算phi(k)v\n",
    "        phi_y_v = torch.bmm(phi_y_flat.transpose(1, 2), v)  # (batch_size, P*D, d_head)\n",
    "        \n",
    "        # 计算phi(k)的归一化因子\n",
    "        phi_y_sum = phi_y_flat.sum(dim=1, keepdim=True)  # (batch_size, 1, P*D)\n",
    "        \n",
    "        return {\n",
    "            'phi_y_flat': phi_y_flat,  # (batch_size, seq_len, P*D)\n",
    "            'phi_y_v': phi_y_v,        # (batch_size, P*D, d_head)\n",
    "            'phi_y_sum': phi_y_sum,    # (batch_size, 1, P*D)\n",
    "            'v': v                      # (batch_size, seq_len, d_head)\n",
    "        }\n",
    "    \n",
    "    def _compute_attention_with_precomputed(self, q, kv_features, head_idx):\n",
    "        \"\"\"使用预计算的kv特征计算注意力\"\"\"\n",
    "        if self.random_weights is None:\n",
    "            self._init_random_features(q.dtype, q.device)\n",
    "        \n",
    "        batch_size, seq_len, _ = q.shape\n",
    "        \n",
    "        # 确保所有计算都使用相同的dtype\n",
    "        X = q / self.shrink\n",
    "        w_flat = self.random_weights[head_idx]\n",
    "        \n",
    "        # 执行计算\n",
    "        proj_x_flat = X @ w_flat.t()\n",
    "        proj_x = proj_x_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        \n",
    "        # 计算phi(q)\n",
    "        phi_x = torch.cumprod(proj_x / self.normalizer, dim=2)\n",
    "        phi_x_flat = phi_x.reshape(batch_size, seq_len, self.P * self.D)\n",
    "        \n",
    "        # 计算phi(q)phi(k)v\n",
    "        attn_output = torch.bmm(phi_x_flat, kv_features['phi_y_v'])  # (batch_size, seq_len, d_head)\n",
    "        \n",
    "        # 计算归一化因子\n",
    "        norm_factor = torch.bmm(phi_x_flat, kv_features['phi_y_sum'].transpose(1, 2))  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        # 应用power和归一化\n",
    "        attn_output = attn_output / (norm_factor + 1e-6)\n",
    "        attn_output = (1.0 + attn_output) ** self.power\n",
    "        \n",
    "        # 最终归一化\n",
    "        attn_output = attn_output / attn_output.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return attn_output\n",
    "        \n",
    "    def _rfa_attention_head(self, q, k, v, head_idx):\n",
    "        \"\"\"计算单个头的RFA注意力\"\"\"\n",
    "        if self.random_weights is None:\n",
    "            self._init_random_features(q.dtype, q.device)\n",
    "        \n",
    "        batch_size, seq_len, _ = q.shape\n",
    "        \n",
    "        # 确保所有计算都使用相同的dtype\n",
    "        X = q / self.shrink\n",
    "        Y = k / self.shrink\n",
    "        w_flat = self.random_weights[head_idx]\n",
    "        \n",
    "        # 执行计算\n",
    "        proj_x_flat = X @ w_flat.t()\n",
    "        proj_y_flat = Y @ w_flat.t()\n",
    "        \n",
    "        proj_x = proj_x_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        proj_y = proj_y_flat.view(batch_size, seq_len, self.P, self.D)\n",
    "        \n",
    "        # 计算phi(k)和phi(q)\n",
    "        phi_x = torch.cumprod(proj_x / self.normalizer, dim=2)\n",
    "        phi_y = torch.cumprod(proj_y / self.normalizer, dim=2)\n",
    "        \n",
    "        # 重塑为(batch_size, seq_len, P*D)\n",
    "        phi_x_flat = phi_x.reshape(batch_size, seq_len, self.P * self.D)\n",
    "        phi_y_flat = phi_y.reshape(batch_size, seq_len, self.P * self.D)\n",
    "        v = v ** 0.01\n",
    "        # 计算phi(k)v\n",
    "        phi_y_v = torch.bmm(phi_y_flat.transpose(1, 2), v)  # (batch_size, P*D, d_head)\n",
    "        \n",
    "        # 计算phi(q)phi(k)v\n",
    "        attn_output = torch.bmm(phi_x_flat, phi_y_v)  # (batch_size, seq_len, d_head)\n",
    "        \n",
    "        # 计算归一化因子\n",
    "        phi_y_sum = phi_y_flat.sum(dim=1, keepdim=True)  # (batch_size, 1, P*D)\n",
    "        norm_factor = torch.bmm(phi_x_flat, phi_y_sum.transpose(1, 2))  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        # 应用power和归一化\n",
    "        attn_output = attn_output / (norm_factor + 1e-6)\n",
    "        attn_output = (1.0 + attn_output) ** self.power\n",
    "        \n",
    "        # 最终归一化\n",
    "        attn_output = attn_output / attn_output.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return attn_output\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):\n",
    "        tgt_len, bsz, embed_dim = query.shape\n",
    "        src_len = key.size(0)\n",
    "        \n",
    "        # 投影\n",
    "        q = self.q_proj(query)  # [tgt_len, bsz, embed_dim]\n",
    "        k = self.k_proj(key)    # [src_len, bsz, embed_dim]\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        # 重塑为多头形式\n",
    "        q = q.reshape(tgt_len, bsz * self.num_heads, self.d_head).transpose(0, 1)\n",
    "        k = k.reshape(src_len, bsz * self.num_heads, self.d_head).transpose(0, 1)\n",
    "        v = v.reshape(src_len, bsz * self.num_heads, self.d_head).transpose(0, 1)\n",
    "        \n",
    "        # 计算每个头的注意力\n",
    "        attn_outputs = []\n",
    "        for h in range(self.num_heads):\n",
    "            # 获取当前头的q、k、v\n",
    "            q_h = q[h*bsz:(h+1)*bsz]  # [bsz, tgt_len, d_head]\n",
    "            k_h = k[h*bsz:(h+1)*bsz]  # [bsz, src_len, d_head]\n",
    "            v_h = v[h*bsz:(h+1)*bsz]  # [bsz, src_len, d_head]\n",
    "            \n",
    "            # 生成缓存键\n",
    "            cache_key = f\"head_{h}_kv\"\n",
    "            \n",
    "            # 检查是否需要预计算kv特征\n",
    "            if cache_key not in self.kv_cache:\n",
    "                self.kv_cache[cache_key] = self._precompute_kv_features(k_h, v_h, h)\n",
    "            \n",
    "            # 使用预计算的特征计算注意力\n",
    "            attn_output = self._compute_attention_with_precomputed(\n",
    "                q_h,\n",
    "                self.kv_cache[cache_key],\n",
    "                h\n",
    "            )\n",
    "            attn_outputs.append(attn_output)\n",
    "        \n",
    "        # 合并多头输出\n",
    "        attn_output = torch.cat(attn_outputs, dim=0)\n",
    "        attn_output = attn_output.transpose(0, 1).reshape(tgt_len, bsz, embed_dim)\n",
    "        \n",
    "        # 输出投影\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2ddb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预热中...\n",
      "\n",
      "开始速度测试...\n",
      "\\Time test:\n",
      "RFA Attention: 0.014 s\n",
      "MultiheadAttention: 0.025 s\n",
      "Ratio: 1.80x\n",
      "\n",
      "开始精度测试...\n",
      "\n",
      "精度测试结果:\n",
      "所有组合的平均相对L2误差: nan\n",
      "所有组合的平均最大绝对误差: nan\n",
      "所有组合的平均平均绝对误差: nan\n",
      "\n",
      "统计信息:\n",
      "总组数: 200\n",
      "总计算次数: 400\n",
      "每组平均计算次数: 2.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "# 1. 加载数据\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_full = torch.load(\"subset_qk/block_1_q_proj_batch_6.pt\", map_location=device)\n",
    "k_full = torch.load(\"subset_qk/block_1_k_proj_batch_6.pt\", map_location=device)\n",
    "\n",
    "sample_size = 4096\n",
    "d_model = 4096\n",
    "num_heads = 32\n",
    "d_head = d_model // num_heads\n",
    "num_q_permutations = 50 # q的排列次数\n",
    "\n",
    "# 计算计数器\n",
    "computation_count = 0\n",
    "group_count = 0\n",
    "\n",
    "def prepare_test_data():\n",
    "    q = q_full[0].float()\n",
    "    k = k_full[0].float()\n",
    "    \n",
    "    # 取出所有head的q\n",
    "    q_all = (q.view(-1, num_heads, d_head)\n",
    "            .permute(1, 0, 2)[:, :sample_size]\n",
    "            .div_(128 ** 0.25)\n",
    "            .to(device))  # [num_heads, sample_size, d_head]\n",
    "    \n",
    "    # 取出所有head的k\n",
    "    k_all = (k.view(-1, num_heads, d_head)\n",
    "            .permute(1, 0, 2)[:, :sample_size]\n",
    "            .div_(128 ** 0.25)\n",
    "            .to(device))  # [num_heads, sample_size, d_head]\n",
    "    \n",
    "    # 生成v\n",
    "    v = ((q_all[0] + k_all[0]) / 2).float()  # [sample_size, d_head]\n",
    "    \n",
    "    # 生成50个不同的q排列\n",
    "    q_permutations = []\n",
    "    for _ in range(num_q_permutations):\n",
    "        perm = torch.randperm(sample_size, device=device)\n",
    "        q_perm = q_all[0][perm]  # 使用第一个head的q进行排列\n",
    "        q_permutations.append(q_perm)\n",
    "    \n",
    "    return q_permutations, k_all[0], v  # 返回q排列列表，固定的k和v\n",
    "\n",
    "# 2. 初始化优化后的RFA和官方MHA\n",
    "optimized_rfa = OptimizedRFAMultiHeadAttention(\n",
    "    d_model=d_head,\n",
    "    num_heads=1,\n",
    "    P=4,\n",
    "    D=100,\n",
    "    shrink=10.0,\n",
    "    power=100.0\n",
    ").to(device)\n",
    "\n",
    "torch_mha = torch.nn.MultiheadAttention(\n",
    "    embed_dim=d_head,\n",
    "    num_heads=1,\n",
    "    batch_first=False,\n",
    "    bias=False\n",
    ").to(device)\n",
    "\n",
    "# 让MHA的权重为单位阵，等价于直接qk^T\n",
    "torch_mha.in_proj_weight.data.copy_(torch.cat([\n",
    "    torch.eye(d_head), torch.eye(d_head), torch.eye(d_head)\n",
    "], dim=0))\n",
    "\n",
    "torch_mha.out_proj.weight.data.copy_(torch.eye(d_head))\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 3. 定义基准函数\n",
    "def benchmark_optimized_rfa():\n",
    "    global computation_count, group_count\n",
    "    q_permutations, k, v = prepare_test_data()\n",
    "    outputs = []\n",
    "    \n",
    "    # 预计算k和v的phi（只计算一次）\n",
    "    k_phi = optimized_rfa._precompute_kv_features(\n",
    "        k.unsqueeze(0),  # [1, sample_size, d_head]\n",
    "        v.unsqueeze(0),  # [1, sample_size, d_head]\n",
    "        head_idx=0\n",
    "    )\n",
    "    \n",
    "    # 对每个q排列计算注意力\n",
    "    for q in q_permutations:\n",
    "        # 计算注意力\n",
    "        out = optimized_rfa._compute_attention_with_precomputed(\n",
    "            q.unsqueeze(0),  # [1, sample_size, d_head]\n",
    "            k_phi,\n",
    "            head_idx=0\n",
    "        ).squeeze(0)  # [sample_size, d_head]\n",
    "        \n",
    "        outputs.append(out)\n",
    "        computation_count += 1\n",
    "        group_count += 1\n",
    "    \n",
    "    return torch.stack(outputs)  # [num_q_permutations, sample_size, d_head]\n",
    "\n",
    "def benchmark_torch_mha():\n",
    "    global computation_count\n",
    "    q_permutations, k, v = prepare_test_data()\n",
    "    outputs = []\n",
    "    \n",
    "    # 对每个q排列计算注意力\n",
    "    for q in q_permutations:\n",
    "        # 计算注意力\n",
    "        out, _ = torch_mha(\n",
    "            q.unsqueeze(1),  # [sample_size, 1, d_head]\n",
    "            k.unsqueeze(1),  # [sample_size, 1, d_head]\n",
    "            v.unsqueeze(1),  # [sample_size, 1, d_head]\n",
    "            need_weights=False\n",
    "        )\n",
    "        outputs.append(out.squeeze(1))  # [sample_size, d_head]\n",
    "        computation_count += 1\n",
    "    \n",
    "    return torch.stack(outputs)  # [num_q_permutations, sample_size, d_head]\n",
    "\n",
    "# 4. 预热\n",
    "print(\"\\n预热中...\")\n",
    "for _ in range(3):\n",
    "    benchmark_optimized_rfa()\n",
    "    benchmark_torch_mha()\n",
    "    clear_memory()\n",
    "\n",
    "# 5. 速度测试\n",
    "print(\"\\n开始速度测试...\")\n",
    "start_time = time.time()\n",
    "optimized_out = benchmark_optimized_rfa()\n",
    "optimized_time = time.time() - start_time\n",
    "clear_memory()\n",
    "\n",
    "start_time = time.time()\n",
    "torch_out = benchmark_torch_mha()\n",
    "torch_time = time.time() - start_time\n",
    "clear_memory()\n",
    "\n",
    "print(f\"\\Time test:\")\n",
    "print(f\"RFA Attention: {optimized_time:.3f} s\")\n",
    "print(f\"MultiheadAttention: {torch_time:.3f} s\")\n",
    "print(f\"Ratio: {torch_time/optimized_time:.2f}x\")\n",
    "# 6. 精度测试\n",
    "print(\"\\n开始精度测试...\")\n",
    "with torch.no_grad():\n",
    "    # 计算每个组合的误差\n",
    "    errors = []\n",
    "    max_abs_errors = []\n",
    "    mean_abs_errors = []\n",
    "    \n",
    "    # 确保使用正确的循环范围\n",
    "    for i in range(num_q_permutations):  # 使用q排列的数量\n",
    "        # 计算相对L2误差\n",
    "        error = torch.norm(optimized_out[i] - torch_out[i]) / torch.norm(torch_out[i])\n",
    "        errors.append(error.item())\n",
    "        \n",
    "        # 计算最大绝对误差\n",
    "        max_abs_error = torch.max(torch.abs(optimized_out[i] - torch_out[i])).item()\n",
    "        max_abs_errors.append(max_abs_error)\n",
    "        \n",
    "        # 计算平均绝对误差\n",
    "        mean_abs_error = torch.mean(torch.abs(optimized_out[i] - torch_out[i])).item()\n",
    "        mean_abs_errors.append(mean_abs_error)\n",
    "    \n",
    "    print(f\"\\n精度测试结果:\")\n",
    "    print(f\"所有组合的平均相对L2误差: {sum(errors)/len(errors):.3e}\")\n",
    "    print(f\"所有组合的平均最大绝对误差: {sum(max_abs_errors)/len(max_abs_errors):.3e}\")\n",
    "    print(f\"所有组合的平均平均绝对误差: {sum(mean_abs_errors)/len(mean_abs_errors):.3e}\")\n",
    "# 7. 统计信息\n",
    "print(f\"\\n统计信息:\")\n",
    "print(f\"总组数: {group_count:,}\")\n",
    "print(f\"总计算次数: {computation_count:,}\")\n",
    "print(f\"每组平均计算次数: {computation_count/group_count:.2f}\")\n",
    "\n",
    "# 最后清理内存\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ea3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 精度测试\n",
    "print(\"\\n开始精度测试...\")\n",
    "with torch.no_grad():\n",
    "    # 计算每个组合的误差\n",
    "    errors = []\n",
    "    max_abs_errors = []\n",
    "    mean_abs_errors = []\n",
    "    \n",
    "    # 确保使用正确的循环范围\n",
    "    for i in range(num_q_permutations):  # 使用q排列的数量\n",
    "        # 计算相对L2误差\n",
    "        error = torch.norm(optimized_out[i] - torch_out[i]) / torch.norm(torch_out[i])\n",
    "        errors.append(error.item())\n",
    "        \n",
    "        # 计算最大绝对误差\n",
    "        max_abs_error = torch.max(torch.abs(optimized_out[i] - torch_out[i])).item()\n",
    "        max_abs_errors.append(max_abs_error)\n",
    "        \n",
    "        # 计算平均绝对误差\n",
    "        mean_abs_error = torch.mean(torch.abs(optimized_out[i] - torch_out[i])).item()\n",
    "        mean_abs_errors.append(mean_abs_error)\n",
    "    \n",
    "    print(f\"\\n精度测试结果:\")\n",
    "    print(f\"所有组合的平均相对L2误差: {sum(errors)/len(errors):.3e}\")\n",
    "    print(f\"所有组合的平均最大绝对误差: {sum(max_abs_errors)/len(max_abs_errors):.3e}\")\n",
    "    print(f\"所有组合的平均平均绝对误差: {sum(mean_abs_errors)/len(mean_abs_errors):.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ff00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
